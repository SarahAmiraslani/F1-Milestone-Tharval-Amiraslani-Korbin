{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Predicting in top 10"]},{"cell_type":"markdown","metadata":{},"source":["## Imports and Configuration"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Suppress warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Standard library imports\n","import sys\n","import os\n","import logging\n","import time\n","\n","# Third-party imports\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from joblib import dump, load\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","# from tensorflow.keras.layers import Dense\n","# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","\n","# User-defined imports\n","module_path = os.path.abspath(os.path.join(\"..\", \"scripts\"))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","\n","from utilities import remove_unnamed_col, convert_to_seconds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Configure logging\n","logging.basicConfig(\n","    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",")"]},{"cell_type":"markdown","metadata":{"cell_id":"879f781ac24949eab9c7446156f2cb8c","deepnote_cell_type":"markdown"},"source":["Below are the columns required for the supervised learning model. Columns marked with an asterisk (*) are indices and won't be included in the model input. Columns marked with a hash (#) are labels.\n","\n","- **Index Columns:**\n","  - *RaceId {year-session} `[race_id]`\n","  - *RacerId {firstname-lastname} `[racer_id]`\n","  - *TrackId {circuitname} `[track_id]`\n","\n","- **Feature Columns:**\n","  - Type of track `[is_street]`\n","    - 0 for purpose-built track\n","    - 1 for street track\n","  - Previous year result for racer `[prev_year_pos]` (0 if not available or didn't participate) \n","  - Qualifying position `[qualifying_pos]` \n","  - Qualifying timing:\n","    - Q1 `[q1_timing]` \n","    - Q2 `[q2_timing]` (if racer didn't qualify for Q2, this will be 0) \n","    - Q3 `[q3_timing]` (if racer didn't qualify for Q3, this will be 0) \n","\n","- **Label Column:**\n","  - #Race finish `[race_finish]` "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["required_columns = [\"driverId\", \"q1\", \"q2\", \"q3\", \"race_id\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_and_process_csv(filepath, id_columns, rename_columns=None, fill_na_value=None):\n","    \"\"\"\n","    Generic function to load and process CSV files.\n","    Args:\n","        filepath (str): The file path to the CSV file.\n","        id_columns (list): Columns to be used to create the unique ID.\n","        rename_columns (dict): Optional dictionary for renaming columns.\n","        fill_na_value (any): Optional value to fill NaN values.\n","    Returns:\n","        pd.DataFrame: Processed DataFrame.\n","    \"\"\"\n","    df = pd.read_csv(filepath)\n","    df[\"race_id\"] = df[id_columns[0]].astype(str) + \"_\" + df[id_columns[1]].astype(str)\n","    if rename_columns:\n","        df = df.rename(columns=rename_columns)\n","    if fill_na_value is not None:\n","        df = df.fillna(fill_na_value)\n","    df = remove_unnamed_col(df)\n","    return df\n","\n","def join_dataframes(df1, df2, join_key):\n","    \"\"\"Join two DataFrames on a specified key.\"\"\"\n","    return df1.merge(df2, how=\"left\", on=join_key)\n","\n","\n","# Load and process data\n","race_information_filepath = \"../data/raw/Race_Information_1995_2023.csv\"\n","race_results_filepath = \"../data/raw/Race_Results_1995_2023.csv\"\n","qualifying_results_filepath = \"../data/raw/Qualification_Results_1995_2023.csv\"\n","\n","race_information = load_and_process_csv(race_information_filepath, [\"season\", \"round\"])\n","race_results = load_and_process_csv(\n","    race_results_filepath,\n","    [\"season\", \"round\"],\n","    rename_columns={\"grid\": \"qualifying_pos\"},\n",")\n","race_results_information = join_dataframes(race_results, race_information, \"race_id\")\n","qualifying_results = load_and_process_csv(\n","    qualifying_results_filepath,\n","    [\"season\", \"round\"],\n","    rename_columns={\"q1\": \"q1_timing\", \"q2\": \"q2_timing\", \"q3\": \"q3_timing\"},\n","    fill_na_value=0,\n",")\n","\n","\n","display(race_results_information.info())\n","display(qualifying_results.info())"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"59d44fabd3104b14b77c13f28c64e35b","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":5,"sortBy":[]},"deepnote_to_be_reexecuted":false,"execution_millis":92556,"execution_start":1709433548716,"source_hash":null},"outputs":[],"source":["def add_previous_year_results(df):\n","    \"\"\"\n","    Adds previous year's average position for each driver to the DataFrame.\n","\n","    Parameters:\n","    - df: pandas DataFrame containing race results information.\n","\n","    Returns:\n","    - Modified DataFrame with an additional column for the previous year's average position.\n","    \"\"\"\n","    # Create a copy of the DataFrame with the season incremented to match the next year's season\n","    prev_year_info = df.copy()\n","    prev_year_info[\"season\"] += 1\n","\n","    # Group by driverId, circuitId, and season, then calculate the mean position\n","    prev_year_avg_positions = (\n","        prev_year_info.groupby([\"driverId\", \"circuitId\", \"season\"])[\"position\"]\n","        .mean()\n","        .reset_index()\n","    )\n","\n","    # Rename columns to match for merging\n","    prev_year_avg_positions.rename(columns={\"position\": \"prev_year_pos\"}, inplace=True)\n","\n","    # Merge the modified DataFrame back to the original DataFrame\n","    df = df.merge(\n","        prev_year_avg_positions, on=[\"driverId\", \"circuitId\", \"season\"], how=\"left\"\n","    )\n","\n","    # Fill NaN values with 0 for drivers without a previous year position\n","    df[\"prev_year_pos\"].fillna(0, inplace=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"69efb3c3903d4317ae6c770b3189643b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":491,"execution_start":1709433641275,"source_hash":null},"outputs":[],"source":["# Assuming you want to keep all matches and all columns from both DataFrames are relevant\n","merged_df = pd.merge(\n","    qualifying_results_required_columns,\n","    race_results_information[\n","        [\n","            \"driverId\",\n","            \"race_id\",\n","            \"position\",\n","            \"qualifying_pos\",\n","            \"prev_year_pos\",\n","            \"season\",\n","            \"round\",\n","        ]\n","    ],\n","    on=[\"driverId\", \"race_id\"],\n","    how=\"inner\",\n","    validate=\"one_to_one\",\n",")\n","\n","# Directly saving the required DataFrame to CSV, no need to create an intermediate DataFrame\n","merged_df.to_csv(\"../data/prepared/Complete_data_supervised_learning.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2e0432a140b743a9a5ad77a184261273","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":92,"execution_start":1709587347198,"source_hash":null},"outputs":[],"source":["# You can also read the data directly (if available)\n","final_data = pd.read_csv(\"Complete_data_supervised_learning.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3825cd3b4c7e4dbf920e72f318829395","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":58,"execution_start":1709587348444,"source_hash":null},"outputs":[],"source":["\n","\n","# Convert timing columns to seconds\n","final_data['q1_timing'] = final_data['q1_timing'].apply(convert_to_seconds)\n","final_data['q2_timing'] = final_data['q2_timing'].apply(convert_to_seconds)\n","final_data['q3_timing'] = final_data['q3_timing'].apply(convert_to_seconds)\n","final_data['position'] = final_data['position'].apply(lambda x: 1 if x <= 10 else 0)\n","final_data_required = final_data[[\"q1_timing\", \"q2_timing\", \"q3_timing\", \"qualifying_pos\", \"prev_year_pos\", \"position\"]]\n","final_data_scaled = final_data_required.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f20e54288900417b9c9803bb8c0e8755","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":431,"execution_start":1709587356712,"source_hash":null},"outputs":[],"source":["scaler = MinMaxScaler()\n","final_data_scaled[['q1_timing', 'q2_timing', 'q3_timing', 'qualifying_pos', 'prev_year_pos']] = scaler.fit_transform(final_data_scaled[['q1_timing', 'q2_timing', 'q3_timing', 'qualifying_pos', 'prev_year_pos']])\n","\n","# Calculate correlation matrix\n","corr_matrix = final_data_scaled.corr()\n","\n","# Plotting the correlation matrix as a heatmap\n","plt.figure(figsize=(20, 10))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"bf2ef54aa3f34c16bac5bab6619ccee2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":292,"execution_start":1709587362138,"source_hash":null},"outputs":[],"source":["# Splitting the dataset into features and the label\n","label_col = \"position\"\n","X = final_data_required.drop(label_col, axis=1)\n","y = final_data_required[label_col]\n","\n","# Splitting the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normalizing the feature data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["## Modeling"]},{"cell_type":"markdown","metadata":{},"source":["### Linear Methods"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_best_logistic_regression_model(\n","    X_train_scaled, X_test_scaled, y_train, y_test, param_grid, **grid_search_kwargs\n","):\n","    \"\"\"\n","    Builds and returns the best Logistic Regression model using GridSearchCV.\n","\n","    Args:\n","        X_train_scaled (pd.DataFrame): Scaled training features.\n","        X_test_scaled (pd.DataFrame): Scaled test features.\n","        y_train (pd.Series): Training labels.\n","        y_test (pd.Series): Test labels.\n","        param_grid (dict): Parameter grid for GridSearchCV.\n","        **grid_search_kwargs: Additional keyword arguments for GridSearchCV.\n","\n","    Returns:\n","        best_model (LogisticRegression): The best Logistic Regression model.\n","        accuracy (float): Accuracy of the best model on the test set.\n","    \"\"\"\n","    # Initializing the Logistic Regression model\n","    log_reg = LogisticRegression(random_state=42, max_iter=1000)\n","\n","    # Setting up GridSearchCV to find the best model\n","    grid_search = GridSearchCV(\n","        estimator=log_reg,\n","        param_grid=param_grid,\n","        cv=5,\n","        scoring=\"accuracy\",\n","        n_jobs=-1,\n","        verbose=1,\n","        **grid_search_kwargs,\n","    )\n","\n","    start_time = time.time()\n","    logging.info(\"Starting GridSearchCV to find the best Logistic Regression model.\")\n","\n","    try:\n","        # Fitting GridSearchCV to the training data\n","        grid_search.fit(X_train_scaled, y_train)\n","\n","        # Extracting the best estimator (model)\n","        best_model = grid_search.best_estimator_\n","\n","        # Making predictions with the best model on the test set\n","        y_pred = best_model.predict(X_test_scaled)\n","\n","        # Calculating the accuracy of the best model\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","\n","        logging.info(f\"GridSearchCV completed in {elapsed_time:.2f} seconds.\")\n","        logging.info(f\"Best Model's Accuracy: {accuracy * 100:.2f}%\")\n","        logging.info(f\"Best Parameters: {grid_search.best_params_}\")\n","\n","        # Returning the best model and its accuracy\n","        return best_model, accuracy\n","\n","    except Exception as e:\n","        logging.error(f\"An error occurred during GridSearchCV: {e}\")\n","        return None, None\n","\n","\n","# Defining the parameter grid for logistic regression\n","lr_param_grid = {\n","    \"C\": [0.01, 0.1, 1, 10, 100],\n","    \"penalty\": [\"l1\", \"l2\"],\n","    \"solver\": [\"liblinear\"],  # 'liblinear' is compatible with l1 and l2 penalties.\n","}\n","\n","# Building and evaluating the Logistic Regression model\n","best_lr_model, best_lr_accuracy = build_best_logistic_regression_model(\n","    X_train_scaled,\n","    X_test_scaled,\n","    y_train,\n","    y_test,\n","    lr_param_grid,\n","    return_train_score=True,\n",")\n","\n","if best_lr_model is not None:\n","    dump(best_lr_model, \"best_logistic_regression_model.joblib\")\n","    # You can also read the model directly if available\n","    # best_lr_model = load(\"best_logistic_regression_model.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["### Ensemble Methods"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"aefab90c280645d3ad1794f9bd986d86","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":141196,"execution_start":1709587488247,"source_hash":null},"outputs":[{"ename":"NameError","evalue":"name 'X_train_scaled' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 73\u001b[0m\n\u001b[1;32m     65\u001b[0m rf_param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m     69\u001b[0m }\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Example usage with additional GridSearchCV parameters\u001b[39;00m\n\u001b[1;32m     72\u001b[0m best_rf_model, best_rf_accuracy \u001b[38;5;241m=\u001b[39m build_best_random_forest_model(\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mX_train_scaled\u001b[49m,\n\u001b[1;32m     74\u001b[0m     X_test_scaled,\n\u001b[1;32m     75\u001b[0m     y_train,\n\u001b[1;32m     76\u001b[0m     y_test,\n\u001b[1;32m     77\u001b[0m     rf_param_grid,\n\u001b[1;32m     78\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_rf_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     dump(best_rf_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_random_forest_model.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"]}],"source":["def build_best_random_forest_model(\n","    X_train, X_test, y_train, y_test, param_grid, **grid_search_kwargs\n","):\n","    \"\"\"\n","    Builds and returns the best Random Forest model using GridSearchCV.\n","\n","    Args:\n","        X_train (pd.DataFrame): Training features.\n","        X_test (pd.DataFrame): Test features.\n","        y_train (pd.Series): Training labels.\n","        y_test (pd.Series): Test labels.\n","        param_grid (dict): Parameter grid for GridSearchCV.\n","        **grid_search_kwargs: Additional keyword arguments for GridSearchCV.\n","\n","    Returns:\n","        best_model (RandomForestClassifier): The best Random Forest model.\n","        accuracy (float): Accuracy of the best model on the test set.\n","    \"\"\"\n","    # Initializing the Random Forest classifier\n","    rf_clf = RandomForestClassifier(random_state=42)\n","\n","    # Setting up GridSearchCV to find the best model\n","    grid_search = GridSearchCV(\n","        estimator=rf_clf,\n","        param_grid=param_grid,\n","        cv=5,\n","        scoring=\"accuracy\",\n","        n_jobs=-1,\n","        verbose=1,\n","        **grid_search_kwargs,\n","    )\n","\n","    start_time = time.time()\n","    logging.info(\"Starting GridSearchCV to find the best Random Forest model.\")\n","\n","    try:\n","        # Fitting GridSearchCV to the training data\n","        grid_search.fit(X_train, y_train)\n","\n","        # Extracting the best estimator (model)\n","        best_model = grid_search.best_estimator_\n","\n","        # Making predictions with the best model on the test set\n","        y_pred = best_model.predict(X_test)\n","\n","        # Calculating the accuracy of the best model\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","\n","        logging.info(f\"GridSearchCV completed in {elapsed_time:.2f} seconds.\")\n","        logging.info(f\"Best Model's Accuracy: {accuracy * 100:.2f}%\")\n","        logging.info(f\"Best Parameters: {grid_search.best_params_}\")\n","\n","        # Returning the best model and its accuracy\n","        return best_model, accuracy\n","\n","    except Exception as e:\n","        logging.error(f\"An error occurred during GridSearchCV: {e}\")\n","        return None, None\n","\n","\n","# Defining the parameter grid for Random Forest\n","rf_param_grid = {\n","    \"n_estimators\": [10, 50, 100, 200],\n","    \"max_depth\": [None, 10, 20, 30],\n","    \"min_samples_split\": [2, 5, 10],\n","}\n","\n","# Example usage with additional GridSearchCV parameters\n","best_rf_model, best_rf_accuracy = build_best_random_forest_model(\n","    X_train_scaled,\n","    X_test_scaled,\n","    y_train,\n","    y_test,\n","    rf_param_grid,\n","    return_train_score=True,\n",")\n","\n","if best_rf_model is not None:\n","    dump(best_rf_model, \"best_random_forest_model.joblib\")\n","    # You can also read the model directly if available\n","    # best_rf_model = load(\"best_random_forest_model.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Network Methods"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"0b999aa56fee47449f662c7d3266416b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":594487,"execution_start":1709587629885,"source_hash":null},"outputs":[],"source":["def create_model(hidden_nodes=1, learning_rate=0.001):\n","    \"\"\"\n","    Creates and compiles a Keras Sequential model.\n","\n","    Args:\n","        hidden_nodes (int): Number of nodes in the hidden layer.\n","        learning_rate (float): Learning rate for the optimizer.\n","\n","    Returns:\n","        model (Sequential): Compiled Keras model.\n","    \"\"\"\n","    # Define the model\n","    model = Sequential()\n","    model.add(Dense(hidden_nodes, input_dim=X_train_scaled.shape[1], activation=\"relu\"))\n","    model.add(Dense(1, activation=\"sigmoid\"))  # Output layer for binary classification\n","\n","    # Compile the model\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n","    return model\n","\n","\n","def build_best_neural_network_model(\n","    X_train_scaled, X_test_scaled, y_train, y_test, param_grid, **grid_search_kwargs\n","):\n","    \"\"\"\n","    Builds and returns the best neural network model using GridSearchCV.\n","\n","    Args:\n","        X_train_scaled (pd.DataFrame): Scaled training features.\n","        X_test_scaled (pd.DataFrame): Scaled test features.\n","        y_train (pd.Series): Training labels.\n","        y_test (pd.Series): Test labels.\n","        param_grid (dict): Parameter grid for GridSearchCV.\n","        **grid_search_kwargs: Additional keyword arguments for GridSearchCV.\n","\n","    Returns:\n","        best_model (KerasClassifier): The best neural network model.\n","        accuracy (float): Accuracy of the best model on the test set.\n","    \"\"\"\n","    # Wrapping the Keras model so it can be used by scikit-learn\n","    model = KerasClassifier(build_fn=create_model, verbose=0)\n","\n","    # Setting up GridSearchCV to find the best model\n","    grid_search = GridSearchCV(\n","        estimator=model,\n","        param_grid=param_grid,\n","        cv=5,\n","        scoring=\"accuracy\",\n","        n_jobs=-1,\n","        verbose=1,\n","        **grid_search_kwargs,\n","    )\n","\n","    start_time = time.time()\n","    logging.info(\"Starting GridSearchCV to find the best neural network model.\")\n","\n","    try:\n","        # Fitting GridSearchCV to the training data\n","        grid_search.fit(X_train_scaled, y_train)\n","\n","        # Extracting the best estimator (model)\n","        best_model = grid_search.best_estimator_\n","\n","        # Making predictions with the best model on the test set\n","        y_pred = best_model.predict(X_test_scaled)\n","        y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary output\n","\n","        # Calculating the accuracy of the best model\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","\n","        logging.info(f\"GridSearchCV completed in {elapsed_time:.2f} seconds.\")\n","        logging.info(f\"Best Model's Accuracy: {accuracy * 100:.2f}%\")\n","        logging.info(f\"Best Parameters: {grid_search.best_params_}\")\n","\n","        # Returning the best model and its accuracy\n","        return best_model, accuracy\n","\n","    except Exception as e:\n","        logging.error(f\"An error occurred during GridSearchCV: {e}\")\n","        return None, None\n","\n","\n","# Defining the parameter grid for the neural network\n","nn_param_grid = {\n","    \"hidden_nodes\": [10, 50, 100],\n","    \"learning_rate\": [0.001, 0.01, 0.1],\n","    \"epochs\": [50],\n","    \"batch_size\": [32],\n","}\n","\n","# Building and evaluating the neural network model\n","best_nn_model, best_nn_accuracy = build_best_neural_network_model(\n","    X_train_scaled,\n","    X_test_scaled,\n","    y_train,\n","    y_test,\n","    nn_param_grid,\n","    return_train_score=True,\n",")\n","\n","if best_nn_model is not None:\n","    dump(best_nn_model, \"best_neural_network_model.joblib\")\n","    # You can also read the model directly if available\n","    # best_nn_model = load(\"best_neural_network_model.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6b49e893c4534bdd88d04adcd70f0e64","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":97327,"execution_start":1709588835781,"source_hash":null},"outputs":[],"source":["def perform_sensitivity_analysis(\n","    models, X_scaled, feature_names, output_dir=\".\", file_format=\"png\"\n","):\n","    \"\"\"\n","    Perform sensitivity analysis on given models.\n","\n","    Args:\n","        models (dict): Dictionary of trained models with their names as keys.\n","        X_scaled (np.ndarray): Scaled features array.\n","        feature_names (list): List of feature names.\n","        output_dir (str): Directory to save the sensitivity analysis plots.\n","        file_format (str): File format for saving plots.\n","    \"\"\"\n","    # Number of points to evaluate for each feature\n","    num_points = 100\n","\n","    # Results dictionary to store sensitivity data\n","    sensitivity_results = {}\n","\n","    # Iterate through each model\n","    for model_name, model in models.items():\n","        logging.info(f\"Analyzing model: {model_name}\")\n","        sensitivity_results[model_name] = {}\n","\n","        # Iterate through each feature\n","        for feature in feature_names:\n","            # Array to hold predictions\n","            predictions = []\n","\n","            # Generate values across the range\n","            feature_index = feature_names.index(feature)\n","            min_val, max_val = np.min(X_scaled[:, feature_index]), np.max(\n","                X_scaled[:, feature_index]\n","            )\n","            values = np.linspace(min_val, max_val, num_points)\n","\n","            # Modify one feature at a time, keeping others constant\n","            for val in values:\n","                X_temp = np.copy(X_scaled)\n","                X_temp[:, feature_index] = val\n","                try:\n","                    pred = model.predict(X_temp)\n","                    predictions.append(np.mean(pred))\n","                except Exception as e:\n","                    logging.error(\n","                        f\"Error predicting with model {model_name} for feature {feature}: {e}\"\n","                    )\n","                    predictions.append(np.nan)  # Use NaN to indicate prediction failure\n","\n","            sensitivity_results[model_name][feature] = np.nanstd(\n","                predictions\n","            )  # Use nanstd to handle NaNs\n","\n","    # Plotting the sensitivity results\n","    for model_name, sensitivities in sensitivity_results.items():\n","        plt.figure(figsize=(10, 6))\n","        plt.title(f\"Sensitivity Analysis for {model_name}\")\n","        plt.bar(range(len(sensitivities)), list(sensitivities.values()), align=\"center\")\n","        plt.xticks(\n","            range(len(sensitivities)), list(sensitivities.keys()), rotation=\"vertical\"\n","        )\n","        plt.ylabel(\"Standard Deviation of Predictions\")\n","\n","        file_path = f\"{output_dir}/sensitivity_{model_name}.{file_format}\"\n","        plt.savefig(file_path, dpi=300)\n","        plt.show()\n","        logging.info(f\"Sensitivity plot saved to {file_path}\")\n","\n","\n","# Example usage\n","models = {\n","    \"Random Forest\": best_rf_model,\n","    \"Logistic Regression\": best_lr_model,\n","    \"Neural Network\": best_nn_model,\n","}\n","\n","feature_names = [\n","    \"q1_timing\",\n","    \"q2_timing\",\n","    \"q3_timing\",\n","    \"qualifying_pos\",\n","    \"prev_year_pos\",\n","]\n","perform_sensitivity_analysis(models, X_test_scaled, feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f90ac10364f649cbba5c6fc8f1480307","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":30395,"execution_start":1709435171245,"source_hash":null},"outputs":[],"source":["# Assuming final_data_required is your DataFrame\n","sns.pairplot(final_data_required, hue='position')\n","plt.savefig('pairplot.png', dpi=300)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fe04c53a9033421ca1b562892325700f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1480,"execution_start":1709434729867,"source_hash":null},"outputs":[],"source":["\n","# Assuming X_train and y_train are already defined\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train_scaled, y_train)\n","\n","# Getting feature importances\n","feature_importances = rf.feature_importances_\n","\n","# Converting to a DataFrame for easier plotting\n","importances_df = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances})\n","\n","# Sorting by importance\n","importances_df = importances_df.sort_values('importance', ascending=False)\n","\n","# Plotting\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='importance', y='feature', data=importances_df)\n","plt.title('Feature Importance')\n","plt.xlabel('Importance')\n","plt.ylabel('Feature')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=347d2df2-e1c6-4f3b-913b-b3a22e587fee' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"f8ecb97b6e994a008c3f46001ea4d527","deepnote_persisted_session":{"createdAt":"2024-03-04T22:49:48.106Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
